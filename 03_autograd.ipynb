{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0326dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413b8d2c",
   "metadata": {},
   "source": [
    "The autograd package provides automatic differentiation\n",
    "for all operations on Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cb7a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires_grad = True -> tracks all operations on the tensor.\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8298ac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y was created as a result of an operation, so it has a grad_fn attribute.\n",
    "# grad_fn: references a Function that has created the Tensor\n",
    "print(x)  # created by the user -> grad_fn is None\n",
    "print(y)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f0d9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do more operations on y\n",
    "z = y * y * 3\n",
    "print(z)\n",
    "z = z.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cdd88d",
   "metadata": {},
   "source": [
    "Let's compute the gradients with backpropagation\n",
    "When we finish our computation we can call .backward() and have all the gradients computed automatically.\n",
    "The gradient for this tensor will be accumulated into .grad attribute.\n",
    "It is the partial derivate of the function w.r.t. the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f42e176",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()\n",
    "print(x.grad)  # dz/dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab489c8b",
   "metadata": {},
   "source": [
    "Generally speaking, torch.autograd is an engine for computing vector-Jacobian product\n",
    "It computes partial derivates while applying the chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573bb37d",
   "metadata": {},
   "source": [
    "-------------\n",
    "Model with non-scalar output:\n",
    "If a Tensor is non-scalar (more than 1 elements), we need to specify arguments for backward()\n",
    "specify a gradient argument that is a tensor of matching shape.\n",
    "needed for vector-Jacobian product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e98b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955c4971",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x * 2\n",
    "for _ in range(10):\n",
    "    y = y * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78ecd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7afa801",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float32)\n",
    "y.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedf7ee4",
   "metadata": {},
   "source": [
    "-------------\n",
    "Stop a tensor from tracking history:\n",
    "For example during our training loop when we want to update our weights\n",
    "then this update operation should not be part of the gradient computation\n",
    "- x.requires_grad_(False)\n",
    "- x.detach()\n",
    "- wrap in 'with torch.no_grad():'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9275ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .requires_grad_(...) changes an existing flag in-place.\n",
    "a = torch.randn(2, 2)\n",
    "print(a.requires_grad)\n",
    "b = (a * 3) / (a - 1)\n",
    "print(b.grad_fn)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d92ad3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .detach(): get a new Tensor with the same content but no gradient computation:\n",
    "a = torch.randn(2, 2, requires_grad=True)\n",
    "print(a.requires_grad)\n",
    "b = a.detach()\n",
    "print(b.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546ef9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap in 'with torch.no_grad():'\n",
    "a = torch.randn(2, 2, requires_grad=True)\n",
    "print(a.requires_grad)\n",
    "with torch.no_grad():\n",
    "    print((x**2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd90f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------\n",
    "# backward() accumulates the gradient for this tensor into .grad attribute.\n",
    "# !!! We need to be careful during optimization !!!\n",
    "# Use .zero_() to empty the gradients before a new optimization step!\n",
    "weights = torch.ones(4, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba8589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(3):\n",
    "    # just a dummy example\n",
    "    model_output = (weights * 3).sum()\n",
    "    model_output.backward()\n",
    "\n",
    "    print(weights.grad)\n",
    "\n",
    "    # optimize model, i.e. adjust weights...\n",
    "    with torch.no_grad():\n",
    "        weights -= 0.1 * weights.grad\n",
    "\n",
    "    # this is important! It affects the final weights & output\n",
    "    weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f01c8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights)\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd57f19d",
   "metadata": {},
   "source": [
    "Optimizer has zero_grad() method\n",
    "optimizer = torch.optim.SGD([weights], lr=0.1)\n",
    "During training:\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
