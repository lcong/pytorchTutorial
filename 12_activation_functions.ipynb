{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0376d8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = w*x + b\n",
    "# output = activation_function(output)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a86a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([-1.0, 1.0, 2.0, 3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7453c850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sofmax\n",
    "output = torch.softmax(x, dim=0)\n",
    "print(output)\n",
    "sm = nn.Softmax(dim=0)\n",
    "output = sm(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950116fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid \n",
    "output = torch.sigmoid(x)\n",
    "print(output)\n",
    "s = nn.Sigmoid()\n",
    "output = s(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839e29be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tanh\n",
    "output = torch.tanh(x)\n",
    "print(output)\n",
    "t = nn.Tanh()\n",
    "output = t(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4700a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu\n",
    "output = torch.relu(x)\n",
    "print(output)\n",
    "relu = nn.ReLU()\n",
    "output = relu(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55317cad",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# leaky relu\n",
    "output = F.leaky_relu(x)\n",
    "print(output)\n",
    "lrelu = nn.LeakyReLU()\n",
    "output = lrelu(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabcb3eb",
   "metadata": {},
   "source": [
    "nn.ReLU() creates an nn.Module which you can add e.g. to an nn.Sequential model.\n",
    "torch.relu on the other side is just the functional API call to the relu function,\n",
    "so that you can add it e.g. in your forward method yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58240150",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# option 1 (create nn modules)\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806586ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 2 (use activation functions directly in forward pass)\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.linear1(x))\n",
    "        out = torch.sigmoid(self.linear2(out))\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
